@inproceedings{Xia_Gladkova_Wang_Li_Stilla_Henriques_Cremers_2023,
address={Paris, France},
title={CASSPR: cross attention single scan place recognition},
rights={https://doi.org/10.15223/policy-029},
ISBN={979-8-3503-0718-4},
url={https://ieeexplore.ieee.org/document/10378588/},
booktitle={2023 IEEE/CVF International Conference on Computer Vision (ICCV)},
publisher={IEEE},
author={Xia, Yan and Gladkova, Mariia and Wang, Rui and Li, Qianyun and Stilla, Uwe and Henriques, João F. and Cremers, Daniel},
year={2023},
month=oct,
pages={8427–8438},
language={en} }

@inproceedings{Wysocki_Xia_Wysocki_Grilli_Hoegner_Cremers_Stilla_2023,
address={Vancouver, BC, Canada},
title={Scan2LoD3: reconstructing semantic 3D building models at LoD3 using ray casting and bayesian networks},
rights={https://doi.org/10.15223/policy-029},
ISBN={979-8-3503-0249-3},
url={https://ieeexplore.ieee.org/document/10208594/},
booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
publisher={IEEE},
author={Wysocki, Olaf and Xia, Yan and Wysocki, Magdalena and Grilli, Eleonora and Hoegner, Ludwig and Cremers, Daniel and Stilla, Uwe},
year={2023},
month=jun,
pages={6548–6558},
language={en} }

@inbook{Wu_Xia_Wan_Chan_2025,
address={Cham},
series={Lecture Notes in Computer Science},
title={Boosting 3D single object tracking with 2D matching distillation and 3D pre-training},
volume={15070},
ISBN={978-3-031-73253-9},
url={https://link.springer.com/10.1007/978-3-031-73254-6_16},
booktitle={Computer Vision – ECCV 2024},
publisher={Springer Nature Switzerland},
author={Wu, Qiangqiang and Xia, Yan and Wan, Jia and Chan, Antoni B.},
editor={Leonardis, Aleš and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, Gül},
year={2025},
pages={270–288},
collection={Lecture Notes in Computer Science},
language={en} }


@misc{yang_ralibev_2024,
	title = {{RaLiBEV}: {Radar} and {LiDAR} {BEV} {Fusion} {Learning} for {Anchor} {Box} {Free} {Object} {Detection} {Systems}},
	shorttitle = {{RaLiBEV}},
	url = {http://arxiv.org/abs/2211.06108},
	doi = {10.48550/arXiv.2211.06108},
	abstract = {In autonomous driving, LiDAR and radar are crucial for environmental perception. LiDAR offers precise 3D spatial sensing information but struggles in adverse weather like fog. Conversely, radar signals can penetrate rain or mist due to their specific wavelength but are prone to noise disturbances. Recent state-of-the-art works reveal that the fusion of radar and LiDAR can lead to robust detection in adverse weather. The existing works adopt convolutional neural network architecture to extract features from each sensor data, then align and aggregate the two branch features to predict object detection results. However, these methods have low accuracy of predicted bounding boxes due to a simple design of label assignment and fusion strategies. In this paper, we propose a bird's-eye view fusion learning-based anchor box-free object detection system, which fuses the feature derived from the radar range-azimuth heatmap and the LiDAR point cloud to estimate possible objects. Different label assignment strategies have been designed to facilitate the consistency between the classification of foreground or background anchor points and the corresponding bounding box regressions. Furthermore, the performance of the proposed object detector is further enhanced by employing a novel interactive transformer module. The superior performance of the methods proposed in this paper has been demonstrated using the recently published Oxford Radar RobotCar dataset. Our system's average precision significantly outperforms the state-of-the-art method by 13.1\% and 19.0\% at Intersection of Union (IoU) of 0.8 under 'Clear+Foggy' training conditions for 'Clear' and 'Foggy' testing, respectively.},
	urldate = {2025-02-24},
	publisher = {arXiv},
	author = {Yang, Yanlong and Liu, Jianan and Huang, Tao and Han, Qing-Long and Ma, Gang and Zhu, Bing},
	month = feb,
	year = {2024},
	note = {20 citations (Semantic Scholar/DOI) [2025-02-24]
arXiv:2211.06108 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, notion},
	annote = {Comment: 14 pages, 6 figures},
	file = {Preprint PDF:E\:\\ZoteroData\\storage\\RZE9LC3T\\Yang et al. - 2024 - RaLiBEV Radar and LiDAR BEV Fusion Learning for Anchor Box Free Object Detection Systems.pdf:application/pdf;Snapshot:E\:\\ZoteroData\\storage\\GFGT8L3S\\2211.html:text/html},
}

@misc{hendy_fishing_2020,
	title = {{FISHING} {Net}: {Future} {Inference} of {Semantic} {Heatmaps} {In} {Grids}},
	shorttitle = {{FISHING} {Net}},
	url = {http://arxiv.org/abs/2006.09917},
	doi = {10.48550/arXiv.2006.09917},
	abstract = {For autonomous robots to navigate a complex environment, it is crucial to understand the surrounding scene both geometrically and semantically. Modern autonomous robots employ multiple sets of sensors, including lidars, radars, and cameras. Managing the different reference frames and characteristics of the sensors, and merging their observations into a single representation complicates perception. Choosing a single unified representation for all sensors simplifies the task of perception and fusion. In this work, we present an end-to-end pipeline that performs semantic segmentation and short term prediction using a top-down representation. Our approach consists of an ensemble of neural networks which take in sensor data from different sensor modalities and transform them into a single common top-down semantic grid representation. We find this representation favorable as it is agnostic to sensor-specific reference frames and captures both the semantic and geometric information for the surrounding scene. Because the modalities share a single output representation, they can be easily aggregated to produce a fused output. In this work we predict short-term semantic grids but the framework can be extended to other tasks. This approach offers a simple, extensible, end-to-end approach for multi-modal perception and prediction.},
	urldate = {2025-02-24},
	publisher = {arXiv},
	author = {Hendy, Noureldin and Sloan, Cooper and Tian, Feng and Duan, Pengfei and Charchut, Nick and Xie, Yuesong and Wang, Chuang and Philbin, James},
	month = jun,
	year = {2020},
	note = {73 citations (Semantic Scholar/arXiv) [2025-02-24]
arXiv:2006.09917 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, notion},
	file = {Preprint PDF:E\:\\ZoteroData\\storage\\P9MTJWY8\\Hendy et al. - 2020 - FISHING Net Future Inference of Semantic Heatmaps In Grids.pdf:application/pdf;Snapshot:E\:\\ZoteroData\\storage\\UJEMSFJV\\2006.html:text/html},
}

@misc{zhou_voxelnet_2017,
	title = {{VoxelNet}: {End}-to-{End} {Learning} for {Point} {Cloud} {Based} {3D} {Object} {Detection}},
	shorttitle = {{VoxelNet}},
	url = {http://arxiv.org/abs/1711.06396},
	doi = {10.48550/arXiv.1711.06396},
	abstract = {Accurate detection of objects in 3D point clouds is a central problem in many applications, such as autonomous navigation, housekeeping robots, and augmented/virtual reality. To interface a highly sparse LiDAR point cloud with a region proposal network (RPN), most existing efforts have focused on hand-crafted feature representations, for example, a bird's eye view projection. In this work, we remove the need of manual feature engineering for 3D point clouds and propose VoxelNet, a generic 3D detection network that unifies feature extraction and bounding box prediction into a single stage, end-to-end trainable deep network. Specifically, VoxelNet divides a point cloud into equally spaced 3D voxels and transforms a group of points within each voxel into a unified feature representation through the newly introduced voxel feature encoding (VFE) layer. In this way, the point cloud is encoded as a descriptive volumetric representation, which is then connected to a RPN to generate detections. Experiments on the KITTI car detection benchmark show that VoxelNet outperforms the state-of-the-art LiDAR based 3D detection methods by a large margin. Furthermore, our network learns an effective discriminative representation of objects with various geometries, leading to encouraging results in 3D detection of pedestrians and cyclists, based on only LiDAR.},
	urldate = {2025-02-24},
	publisher = {arXiv},
	author = {Zhou, Yin and Tuzel, Oncel},
	month = nov,
	year = {2017},
	note = {arXiv:1711.06396 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	file = {Preprint PDF:E\:\\ZoteroData\\storage\\AQAKWDGE\\Zhou and Tuzel - 2017 - VoxelNet End-to-End Learning for Point Cloud Based 3D Object Detection.pdf:application/pdf;Snapshot:E\:\\ZoteroData\\storage\\ND4YVGGY\\1711.html:text/html},
}

@misc{popov_nvradarnet_2023,
	title = {{NVRadarNet}: {Real}-{Time} {Radar} {Obstacle} and {Free} {Space} {Detection} for {Autonomous} {Driving}},
	shorttitle = {{NVRadarNet}},
	url = {http://arxiv.org/abs/2209.14499},
	doi = {10.48550/arXiv.2209.14499},
	abstract = {Detecting obstacles is crucial for safe and efficient autonomous driving. To this end, we present NVRadarNet, a deep neural network (DNN) that detects dynamic obstacles and drivable free space using automotive RADAR sensors. The network utilizes temporally accumulated data from multiple RADAR sensors to detect dynamic obstacles and compute their orientation in a top-down bird's-eye view (BEV). The network also regresses drivable free space to detect unclassified obstacles. Our DNN is the first of its kind to utilize sparse RADAR signals in order to perform obstacle and free space detection in real time from RADAR data only. The network has been successfully used for perception on our autonomous vehicles in real self-driving scenarios. The network runs faster than real time on an embedded GPU and shows good generalization across geographic regions.},
	urldate = {2025-02-24},
	publisher = {arXiv},
	author = {Popov, Alexander and Gebhardt, Patrik and Chen, Ke and Oldja, Ryan and Lee, Heeseok and Murray, Shane and Bhargava, Ruchi and Smolyanskiy, Nikolai},
	month = mar,
	year = {2023},
	note = {22 citations (Semantic Scholar/arXiv) [2025-02-24]
arXiv:2209.14499 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: 7 pages, 6 figures, ICRA 2023 conference, for associated video file, see https://youtu.be/WlwJJMltoJY},
	file = {Preprint PDF:E\:\\ZoteroData\\storage\\PK3I2K9S\\Popov et al. - 2023 - NVRadarNet Real-Time Radar Obstacle and Free Space Detection for Autonomous Driving.pdf:application/pdf;Snapshot:E\:\\ZoteroData\\storage\\FD2YH4WI\\2209.html:text/html},
}


@misc{ding_radarocc_2024,
	title = {{RadarOcc}: {Robust} {3D} {Occupancy} {Prediction} with {4D} {Imaging} {Radar}},
	shorttitle = {{RadarOcc}},
	url = {http://arxiv.org/abs/2405.14014},
	doi = {10.48550/arXiv.2405.14014},
	abstract = {3D occupancy-based perception pipeline has significantly advanced autonomous driving by capturing detailed scene descriptions and demonstrating strong generalizability across various object categories and shapes. Current methods predominantly rely on LiDAR or camera inputs for 3D occupancy prediction. These methods are susceptible to adverse weather conditions, limiting the all-weather deployment of self-driving cars. To improve perception robustness, we leverage the recent advances in automotive radars and introduce a novel approach that utilizes 4D imaging radar sensors for 3D occupancy prediction. Our method, RadarOcc, circumvents the limitations of sparse radar point clouds by directly processing the 4D radar tensor, thus preserving essential scene details. RadarOcc innovatively addresses the challenges associated with the voluminous and noisy 4D radar data by employing Doppler bins descriptors, sidelobe-aware spatial sparsification, and range-wise self-attention mechanisms. To minimize the interpolation errors associated with direct coordinate transformations, we also devise a spherical-based feature encoding followed by spherical-to-Cartesian feature aggregation. We benchmark various baseline methods based on distinct modalities on the public K-Radar dataset. The results demonstrate RadarOcc's state-of-the-art performance in radar-based 3D occupancy prediction and promising results even when compared with LiDAR- or camera-based methods. Additionally, we present qualitative evidence of the superior performance of 4D radar in adverse weather conditions and explore the impact of key pipeline components through ablation studies.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Ding, Fangqiang and Wen, Xiangyu and Zhu, Yunzhou and Li, Yiming and Lu, Chris Xiaoxuan},
	month = oct,
	year = {2024},
	note = {5 citations (Semantic Scholar/arXiv) [2025-01-07]
arXiv:2405.14014 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, notion},
	annote = {Comment: 22 pages, 5 figures, 8 tables. Accepted by NeurIPS 2024 (Vancouver), the Thirty-Eighth Annual Conference on Neural Information Processing Systems},
	file = {Full Text PDF:E\:\\ZoteroData\\storage\\IYINY96Y\\Ding et al. - 2024 - RadarOcc Robust 3D Occupancy Prediction with 4D Imaging Radar.pdf:application/pdf;Snapshot:E\:\\ZoteroData\\storage\\VPBWY58U\\2405.html:text/html},
}


@misc{tian_occ3d_2023,
	title = {{Occ3D}: a large-scale {3D} occupancy prediction benchmark for autonomous driving},
	shorttitle = {{Occ3D}},
	url = {http://arxiv.org/abs/2304.14365},
	doi = {10.48550/arXiv.2304.14365},
	abstract = {Robotic perception requires the modeling of both 3D geometry and semantics. Existing methods typically focus on estimating 3D bounding boxes, neglecting finer geometric details and struggling to handle general, out-of-vocabulary objects. 3D occupancy prediction, which estimates the detailed occupancy states and semantics of a scene, is an emerging task to overcome these limitations. To support 3D occupancy prediction, we develop a label generation pipeline that produces dense, visibility-aware labels for any given scene. This pipeline comprises three stages: voxel densification, occlusion reasoning, and image-guided voxel refinement. We establish two benchmarks, derived from the Waymo Open Dataset and the nuScenes Dataset, namely Occ3D-Waymo and Occ3D-nuScenes benchmarks. Furthermore, we provide an extensive analysis of the proposed dataset with various baseline models. Lastly, we propose a new model, dubbed Coarse-to-Fine Occupancy (CTF-Occ) network, which demonstrates superior performance on the Occ3D benchmarks. The code, data, and benchmarks are released at https://tsinghua-mars-lab.github.io/Occ3D/.},
	language = {en},
	urldate = {2025-02-26},
	publisher = {arXiv},
	author = {Tian, Xiaoyu and Jiang, Tao and Yun, Longfei and Mao, Yucheng and Yang, Huitong and Wang, Yue and Wang, Yilun and Zhao, Hang},
	month = dec,
	year = {2023},
	note = {arXiv:2304.14365 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to NeurIPS 2023},
	file = {Preprint PDF:E\:\\ZoteroData\\storage\\TUNBQMRH\\Tian et al. - 2023 - Occ3D A Large-Scale 3D Occupancy Prediction Benchmark for Autonomous Driving.pdf:application/pdf;Snapshot:E\:\\ZoteroData\\storage\\LCCY6TM6\\2304.html:text/html},
}

@misc{wei_surroundocc_2023,
	title = {{SurroundOcc}: multi-camera {3D} occupancy prediction for autonomous driving},
	shorttitle = {{SurroundOcc}},
	url = {http://arxiv.org/abs/2303.09551},
	doi = {10.48550/arXiv.2303.09551},
	abstract = {3D scene understanding plays a vital role in vision-based autonomous driving. While most existing methods focus on 3D object detection, they have difficulty describing real-world objects of arbitrary shapes and infinite classes. Towards a more comprehensive perception of a 3D scene, in this paper, we propose a SurroundOcc method to predict the 3D occupancy with multi-camera images. We first extract multi-scale features for each image and adopt spatial 2D-3D attention to lift them to the 3D volume space. Then we apply 3D convolutions to progressively upsample the volume features and impose supervision on multiple levels. To obtain dense occupancy prediction, we design a pipeline to generate dense occupancy ground truth without expansive occupancy annotations. Specifically, we fuse multi-frame LiDAR scans of dynamic objects and static scenes separately. Then we adopt Poisson Reconstruction to fill the holes and voxelize the mesh to get dense occupancy labels. Extensive experiments on nuScenes and SemanticKITTI datasets demonstrate the superiority of our method. Code and dataset are available at https://github.com/weiyithu/SurroundOcc},
	language = {en},
	urldate = {2025-02-23},
	publisher = {arXiv},
	author = {Wei, Yi and Zhao, Linqing and Zheng, Wenzhao and Zhu, Zheng and Zhou, Jie and Lu, Jiwen},
	month = aug,
	year = {2023},
	note = {161 citations (Semantic Scholar/arXiv) [2025-02-23]
arXiv:2303.09551 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to ICCV 2023. Code is available at https://github.com/weiyithu/SurroundOcc},
	file = {Preprint PDF:E\:\\ZoteroData\\storage\\2ZEPRGBP\\Wei et al. - 2023 - SurroundOcc Multi-Camera 3D Occupancy Prediction for Autonomous Driving.pdf:application/pdf;Snapshot:E\:\\ZoteroData\\storage\\KRZXPPNH\\2303.html:text/html},
}

@inproceedings{li_st_mvdnetpp_2023,
	title = {{ST}-{MVDNet}++: {Improve} {Vehicle} {Detection} with {Lidar}-{Radar} {Geometrical} {Augmentation} via {Self}-{Training}},
	shorttitle = {{ST}-{MVDNet}++},
	url = {https://ieeexplore.ieee.org/abstract/document/10096041},
	doi = {10.1109/ICASSP49357.2023.10096041},
	abstract = {We aim to improve the performance of the vehicle detection model with Lidar-Radar fusion and data augmentation. The recent works for Lidar-Radar fusion such as MVDNet or ST-MVDNet, have been proposed to have effective performance in detecting vehicles, and address the issue regarding missing modality. However, there are few works applying some global data augmentations such as rotation, translation, and scaling which are common for Lidar-only model. In order to further improve the previous Lidar-Radar fusion model, we propose a model named ST-MVDNet++ by leveraging the self-training teacher-student framework with integrating more common data augmentations such as global rotation, translation, and scaling. To ensure the data augmentations are consistent and matched across Lidar and Radar, we apply the augmentations on bird-eye-view coordinates. We also introduce the student-only augmentation for robust training of the student model with the consistency loss from teacher model. We demonstrate that our leveraging of global consistent Lidar-Radar augmentation improve the previous works by 1 ∼ 2\% in all of the experimental settings.},
	urldate = {2025-02-24},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Li, Yu-Jhe and O’Toole, Matthew and Kitani, Kris},
	month = jun,
	year = {2023},
	note = {5 citations (Semantic Scholar/DOI) [2025-02-24]
ISSN: 2379-190X},
	keywords = {Sensor fusion, Vehicle detection, Laser radar, Sensors, Autonomous Driving, Training, Data models, Radar, Signal processing, Data Augmentation, Self-Training, Sensor-Fusion, notion},
	pages = {1--5},
	file = {Full Text PDF:E\:\\ZoteroData\\storage\\3LJAJRHY\\Li et al. - 2023 - ST-MVDNet++ Improve Vehicle Detection with Lidar-Radar Geometrical Augmentation via Self-Training.pdf:application/pdf;IEEE Xplore Abstract Record:E\:\\ZoteroData\\storage\\YXEUDF5M\\10096041.html:text/html},
}

@inproceedings{qian_mvdnet_2021,
	title = {Robust multimodal vehicle detection in foggy weather using complementary lidar and radar signals},
	url = {https://ieeexplore.ieee.org/document/9578621},
	doi = {10.1109/CVPR46437.2021.00051},
	abstract = {Vehicle detection with visual sensors like lidar and camera is one of the critical functions enabling autonomous driving. While they generate fine-grained point clouds or high-resolution images with rich information in good weather conditions, they fail in adverse weather (e.g., fog) where opaque particles distort lights and significantly reduce visibility. Thus, existing methods relying on lidar or camera experience significant performance degradation in rare but critical adverse weather conditions. To remedy this, we resort to exploiting complementary radar, which is less impacted by adverse weather and becomes prevalent on vehicles. In this paper, we present Multimodal Vehicle Detection Network (MVDNet), a two-stage deep fusion detector, which first generates proposals from two sensors and then fuses region-wise features between multimodal sensor streams to improve final detection results. To evaluate MVDNet, we create a procedurally generated training dataset based on the collected raw lidar and radar signals from the open-source Oxford Radar Robotcar. We show that the proposed MVDNet surpasses other state-of-the-art methods, notably in terms of Average Precision (AP), especially in adverse weather conditions. The code and data are available at https://github.com/qiank10/MVDNet.},
	language = {en},
	urldate = {2025-02-26},
	booktitle = {2021 {Ieee}/cvf {Conference} on {Computer} {Vision} and {Pattern} {Recognition} (cvpr)},
	author = {Qian, Kun and Zhu, Shilin and Zhang, Xinyu and Li, Li Erran},
	month = jun,
	year = {2021},
	note = {ISSN: 2575-7075},
	keywords = {Cameras, Laser radar, Meteorological radar, Sensor phenomena and characterization, Training, Vehicle detection, Visualization},
	pages = {444--453},
	file = {Full Text PDF:E\:\\ZoteroData\\storage\\4A2UBGSM\\Qian et al. - 2021 - Robust Multimodal Vehicle Detection in Foggy Weather Using Complementary Lidar and Radar Signals.pdf:application/pdf;IEEE Xplore Abstract Record:E\:\\ZoteroData\\storage\\5FFX64MX\\9578621.html:text/html},
}

@inproceedings{li_st_mvdnet_2022,
	address = {New Orleans, LA, USA},
	title = {Modality-agnostic learning for radar-lidar fusion in vehicle detection},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-6654-6946-3},
	url = {https://ieeexplore.ieee.org/document/9879704/},
	doi = {10.1109/CVPR52688.2022.00099},
	abstract = {Fusion of multiple sensor modalities such as camera, Lidar, and Radar, which are commonly found on autonomous vehicles, not only allows for accurate detection but also robustifies perception against adverse weather conditions and individual sensor failures. Due to inherent sensor characteristics, Radar performs well under extreme weather conditions (snow, rain, fog) that significantly degrade camera and Lidar. Recently, a few works have developed vehicle detection methods fusing Lidar and Radar signals, i.e., MVDNet. However, these models are typically developed under the assumption that the models always have access to two error-free sensor streams. If one of the sensors is unavailable or missing, the model may fail catastrophically. To mitigate this problem, we propose the Self-Training Multimodal Vehicle Detection Network (ST-MVDNet) which leverages a Teacher-Student mutual learning framework and a simulated sensor noise model used in strong data augmentation for Lidar and Radar. We show that by (1) enforcing output consistency between a Teacher network and a Student network and by (2) introducing missing modalities (strong augmentations) during training, our learned model breaks away from the error-free sensor assumption. This consistency enforcement enables the Student model to handle missing data properly and improve the Teacher model by updating it with the Student model’s exponential moving average. Our experiments demonstrate that our proposed learning framework for multi-modal detection is able to better handle missing sensor data during inference. Furthermore, our method achieves new state-of-the-art performance (5\% gain) on the Oxford Radar Robotcar dataset under various evaluation settings.},
	language = {en},
	urldate = {2025-02-26},
	booktitle = {2022 {Ieee}/cvf {Conference} on {Computer} {Vision} and {Pattern} {Recognition} (cvpr)},
	publisher = {IEEE},
	author = {Li, Yu-Jhe and Park, Jinhyung and O'Toole, Matthew and Kitani, Kris},
	month = jun,
	year = {2022},
	pages = {908--917},
	file = {PDF:E\:\\ZoteroData\\storage\\2DPPEQGL\\Li et al. - 2022 - Modality-Agnostic Learning for Radar-Lidar Fusion in Vehicle Detection.pdf:application/pdf},
}

@inproceedings{chen_multi_view_2017,
	address = {Honolulu, HI},
	title = {Multi-view {3D} {Object} {Detection} {Network} for {Autonomous} {Driving}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8100174/},
	doi = {10.1109/CVPR.2017.691},
	abstract = {This paper aims at high-accuracy 3D object detection in autonomous driving scenario. We propose Multi-View 3D networks (MV3D), a sensory-fusion framework that takes both LIDAR point cloud and RGB images as input and predicts oriented 3D bounding boxes. We encode the sparse 3D point cloud with a compact multi-view representation. The network is composed of two subnetworks: one for 3D object proposal generation and another for multi-view feature fusion. The proposal network generates 3D candidate boxes efﬁciently from the bird’s eye view representation of 3D point cloud. We design a deep fusion scheme to combine region-wise features from multiple views and enable interactions between intermediate layers of different paths. Experiments on the challenging KITTI benchmark show that our approach outperforms the state-of-the-art by around 25\% and 30\% AP on the tasks of 3D localization and 3D detection. In addition, for 2D detection, our approach obtains 14.9\% higher AP than the state-of-the-art on the hard data among the LIDAR-based methods.},
	language = {en},
	urldate = {2025-01-10},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Chen, Xiaozhi and Ma, Huimin and Wan, Ji and Li, Bo and Xia, Tian},
	month = jul,
	year = {2017},
	keywords = {notion},
	pages = {6526--6534},
	file = {PDF:E\:\\ZoteroData\\storage\\HDX7ZYVH\\Chen et al. - 2017 - Multi-view 3D Object Detection Network for Autonomous Driving.pdf:application/pdf},
}

@article{dequaire_deep_2018,
	title = {Deep tracking in the wild: {End}-to-end tracking using recurrent neural networks},
	volume = {37},
	issn = {0278-3649},
	shorttitle = {Deep tracking in the wild},
	url = {https://doi.org/10.1177/0278364917710543},
	doi = {10.1177/0278364917710543},
	abstract = {This paper presents a novel approach for tracking static and dynamic objects for an autonomous vehicle operating in complex urban environments. Whereas traditional approaches for tracking often feature numerous hand-engineered stages, this method is learned end-to-end and can directly predict a fully unoccluded occupancy grid from raw laser input. We employ a recurrent neural network to capture the state and evolution of the environment, and train the model in an entirely unsupervised manner. In doing so, our use case compares to model-free, multi-object tracking although we do not explicitly perform the underlying data-association process. Further, we demonstrate that the underlying representation learned for the tracking task can be leveraged via inductive transfer to train an object detector in a data efficient manner. We motivate a number of architectural features and show the positive contribution of dilated convolutions, dynamic and static memory units to the task of tracking and classifying complex dynamic scenes through full occlusion. Our experimental results illustrate the ability of the model to track cars, buses, pedestrians, and cyclists from both moving and stationary platforms. Further, we compare and contrast the approach with a more traditional model-free multi-object tracking pipeline, demonstrating that it can more accurately predict future states of objects from current inputs.},
	language = {en},
	number = {4-5},
	urldate = {2024-03-15},
	journal = {The International Journal of Robotics Research},
	author = {Dequaire, Julie and Ondrúška, Peter and Rao, Dushyant and Wang, Dominic and Posner, Ingmar},
	month = apr,
	year = {2018},
	note = {64 citations (Crossref) [2024-08-21]
Publisher: SAGE Publications Ltd STM},
	keywords = {/unread, notion},
	pages = {492--512},
	annote = {Focus more on tracking where OGMs are the input rather than proposing new method on generating OGMs

},
	file = {SAGE PDF Full Text:E\:\\ZoteroData\\storage\\XTQBBB82\\Dequaire et al. - 2018 - Deep tracking in the wild End-to-end tracking usi.pdf:application/pdf},
}

@misc{ku_joint_2018,
	title = {Joint {3D} {Proposal} {Generation} and {Object} {Detection} from {View} {Aggregation}},
	url = {http://arxiv.org/abs/1712.02294},
	doi = {10.48550/arXiv.1712.02294},
	abstract = {We present AVOD, an Aggregate View Object Detection network for autonomous driving scenarios. The proposed neural network architecture uses LIDAR point clouds and RGB images to generate features that are shared by two subnetworks: a region proposal network (RPN) and a second stage detector network. The proposed RPN uses a novel architecture capable of performing multimodal feature fusion on high resolution feature maps to generate reliable 3D object proposals for multiple object classes in road scenes. Using these proposals, the second stage detection network performs accurate oriented 3D bounding box regression and category classification to predict the extents, orientation, and classification of objects in 3D space. Our proposed architecture is shown to produce state of the art results on the KITTI 3D object detection benchmark while running in real time with a low memory footprint, making it a suitable candidate for deployment on autonomous vehicles. Code is at: https://github.com/kujason/avod},
	urldate = {2025-02-24},
	publisher = {arXiv},
	author = {Ku, Jason and Mozifian, Melissa and Lee, Jungwook and Harakeh, Ali and Waslander, Steven},
	month = jul,
	year = {2018},
	note = {arXiv:1712.02294 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: For any inquiries contact aharakeh(at)uwaterloo(dot)ca},
	file = {Preprint PDF:E\:\\ZoteroData\\storage\\WTPEURDV\\Ku et al. - 2018 - Joint 3D Proposal Generation and Object Detection from View Aggregation.pdf:application/pdf;Snapshot:E\:\\ZoteroData\\storage\\MNJMKGSX\\1712.html:text/html},
}

@misc{bauer_deep_2019,
	title = {Deep, spatially coherent {Inverse} {Sensor} {Models} with {Uncertainty} {Incorporation} using the evidential {Framework}},
	url = {http://arxiv.org/abs/1904.00842},
	doi = {10.48550/arXiv.1904.00842},
	abstract = {To perform high speed tasks, sensors of autonomous cars have to provide as much information in as few time steps as possible. However, radars, one of the sensor modalities autonomous cars heavily rely on, often only provide sparse, noisy detections. These have to be accumulated over time to reach a high enough confidence about the static parts of the environment. For radars, the state is typically estimated by accumulating inverse detection models (IDMs). We employ the recently proposed evidential convolutional neural networks which, in contrast to IDMs, compute dense, spatially coherent inference of the environment state. Moreover, these networks are able to incorporate sensor noise in a principled way which we further extend to also incorporate model uncertainty. We present experimental results that show This makes it possible to obtain a denser environment perception in fewer time steps.},
	urldate = {2025-01-10},
	publisher = {arXiv},
	author = {Bauer, Daniel and Kuhnert, Lars and Eckstein, Lutz},
	month = mar,
	year = {2019},
	note = {12 citations (Semantic Scholar/arXiv) [2025-01-10]
arXiv:1904.00842 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, notion},
	annote = {Comment: Submitted for Intelligent Vehicle Symposium 2019},
	file = {Preprint PDF:E\:\\ZoteroData\\storage\\U6KMPTUR\\Bauer et al. - 2019 - Deep, spatially coherent Inverse Sensor Models with Uncertainty Incorporation using the evidential F.pdf:application/pdf;Snapshot:E\:\\ZoteroData\\storage\\BAH74W7Q\\1904.html:text/html},
}

@misc{lang_pointpillars_2019,
	title = {{PointPillars}: {Fast} {Encoders} for {Object} {Detection} from {Point} {Clouds}},
	shorttitle = {{PointPillars}},
	url = {http://arxiv.org/abs/1812.05784},
	doi = {10.48550/arXiv.1812.05784},
	abstract = {Object detection in point clouds is an important aspect of many robotics applications such as autonomous driving. In this paper we consider the problem of encoding a point cloud into a format appropriate for a downstream detection pipeline. Recent literature suggests two types of encoders; fixed encoders tend to be fast but sacrifice accuracy, while encoders that are learned from data are more accurate, but slower. In this work we propose PointPillars, a novel encoder which utilizes PointNets to learn a representation of point clouds organized in vertical columns (pillars). While the encoded features can be used with any standard 2D convolutional detection architecture, we further propose a lean downstream network. Extensive experimentation shows that PointPillars outperforms previous encoders with respect to both speed and accuracy by a large margin. Despite only using lidar, our full detection pipeline significantly outperforms the state of the art, even among fusion methods, with respect to both the 3D and bird's eye view KITTI benchmarks. This detection performance is achieved while running at 62 Hz: a 2 - 4 fold runtime improvement. A faster version of our method matches the state of the art at 105 Hz. These benchmarks suggest that PointPillars is an appropriate encoding for object detection in point clouds.},
	urldate = {2025-02-24},
	publisher = {arXiv},
	author = {Lang, Alex H. and Vora, Sourabh and Caesar, Holger and Zhou, Lubing and Yang, Jiong and Beijbom, Oscar},
	month = may,
	year = {2019},
	note = {arXiv:1812.05784 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 9 pages. v1 is initial submission to CVPR 2019. v2 is final version accepted for publication at CVPR 2019},
	file = {Preprint PDF:E\:\\ZoteroData\\storage\\CQXYFM4G\\Lang et al. - 2019 - PointPillars Fast Encoders for Object Detection from Point Clouds.pdf:application/pdf;Snapshot:E\:\\ZoteroData\\storage\\A9439LDS\\1812.html:text/html},
}

@inproceedings{sless_road_2019,
	title = {Road {Scene} {Understanding} by {Occupancy} {Grid} {Learning} from {Sparse} {Radar} {Clusters} using {Semantic} {Segmentation}},
	doi = {10.1109/ICCVW.2019.00115},
	abstract = {Occupancy grid mapping is an important component in road scene understanding for autonomous driving. It encapsulates information of the drivable area, road obstacles and enables safe autonomous driving. Radars are an emerging sensor in autonomous vehicle vision, becoming more widely used due to their long range sensing, low cost, and robustness to severe weather conditions. Despite recent advances in deep learning technology, occupancy grid mapping from radar data is still mostly done using classical filtering approaches. In this work, we propose learning the inverse sensor model used for occupancy grid mapping from clustered radar data. This is done in a data driven approach that leverages computer vision techniques. This task is very challenging due to data sparsity and noise characteristics of the radar sensor. The problem is formulated as a semantic segmentation task and we show how it can be learned using lidar data for generating ground truth. We show both qualitatively and quantitatively that our learned occupancy net outperforms classic methods by a large margin using the recently released NuScenes real-world driving data.},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} {Workshop} ({ICCVW})},
	author = {Sless, Liat and Shlomo, Bat El and Cohen, Gilad and Oron, Shaul},
	month = oct,
	year = {2019},
	note = {44 citations (Crossref) [2024-08-21]
ISSN: 2473-9944},
	keywords = {lidar, machine learning, radar, important, notion},
	pages = {867--875},
	file = {IEEE Xplore Abstract Record:E\:\\ZoteroData\\storage\\M5KMLI4F\\9022091.html:text/html;IEEE Xplore Full Text PDF:E\:\\ZoteroData\\storage\\AI4QSUC4\\Sless et al. - 2019 - Road Scene Understanding by Occupancy Grid Learnin.pdf:application/pdf},
}

@misc{weston_probably_2019,
	title = {Probably {Unknown}: {Deep} {Inverse} {Sensor} {Modelling} {In} {Radar}},
	shorttitle = {Probably {Unknown}},
	url = {http://arxiv.org/abs/1810.08151},
	doi = {10.48550/arXiv.1810.08151},
	abstract = {Radar presents a promising alternative to lidar and vision in autonomous vehicle applications, able to detect objects at long range under a variety of weather conditions. However, distinguishing between occupied and free space from raw radar power returns is challenging due to complex interactions between sensor noise and occlusion. To counter this we propose to learn an Inverse Sensor Model (ISM) converting a raw radar scan to a grid map of occupancy probabilities using a deep neural network. Our network is self-supervised using partial occupancy labels generated by lidar, allowing a robot to learn about world occupancy from past experience without human supervision. We evaluate our approach on five hours of data recorded in a dynamic urban environment. By accounting for the scene context of each grid cell our model is able to successfully segment the world into occupied and free space, outperforming standard CFAR filtering approaches. Additionally by incorporating heteroscedastic uncertainty into our model formulation, we are able to quantify the variance in the uncertainty throughout the sensor observation. Through this mechanism we are able to successfully identify regions of space that are likely to be occluded.},
	urldate = {2025-01-10},
	publisher = {arXiv},
	author = {Weston, Rob and Cen, Sarah and Newman, Paul and Posner, Ingmar},
	month = may,
	year = {2019},
	note = {arXiv:1810.08151 [cs]},
	keywords = {Computer Science - Robotics, notion},
	annote = {Comment: 6 full pages, 1 page of references},
	file = {Preprint PDF:E\:\\ZoteroData\\storage\\E5JBJCSB\\Weston et al. - 2019 - Probably Unknown Deep Inverse Sensor Modelling In Radar.pdf:application/pdf;Snapshot:E\:\\ZoteroData\\storage\\3W6WXQB2\\1810.html:text/html},
}

@misc{roddick_predicting_2020,
	title = {Predicting {Semantic} {Map} {Representations} from {Images} using {Pyramid} {Occupancy} {Networks}},
	url = {http://arxiv.org/abs/2003.13402},
	doi = {10.48550/arXiv.2003.13402},
	abstract = {Autonomous vehicles commonly rely on highly detailed birds-eye-view maps of their environment, which capture both static elements of the scene such as road layout as well as dynamic elements such as other cars and pedestrians. Generating these map representations on the fly is a complex multi-stage process which incorporates many important vision-based elements, including ground plane estimation, road segmentation and 3D object detection. In this work we present a simple, unified approach for estimating maps directly from monocular images using a single end-to-end deep learning architecture. For the maps themselves we adopt a semantic Bayesian occupancy grid framework, allowing us to trivially accumulate information over multiple cameras and timesteps. We demonstrate the effectiveness of our approach by evaluating against several challenging baselines on the NuScenes and Argoverse datasets, and show that we are able to achieve a relative improvement of 9.1\% and 22.3\% respectively compared to the best-performing existing method.},
	urldate = {2024-04-11},
	publisher = {arXiv},
	author = {Roddick, Thomas and Cipolla, Roberto},
	month = mar,
	year = {2020},
	note = {arXiv:2003.13402 [cs]},
	keywords = {/unread, 2bread, Computer Science - Computer Vision and Pattern Recognition, notion},
	file = {arXiv Fulltext PDF:E\:\\ZoteroData\\storage\\5J9I4IES\\Roddick and Cipolla - 2020 - Predicting Semantic Map Representations from Image.pdf:application/pdf;arXiv.org Snapshot:E\:\\ZoteroData\\storage\\X8RDAVR6\\2003.html:text/html},
}

@inproceedings{wu_motionnet_2020,
	address = {Seattle, WA, USA},
	title = {{MotionNet}: {Joint} {Perception} and {Motion} {Prediction} for {Autonomous} {Driving} {Based} on {Bird}’s {Eye} {View} {Maps}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-7281-7168-5},
	shorttitle = {{MotionNet}},
	url = {https://ieeexplore.ieee.org/document/9157538/},
	doi = {10.1109/CVPR42600.2020.01140},
	abstract = {The ability to reliably perceive the environmental states, particularly the existence of objects and their motion behavior, is crucial for autonomous driving. In this work, we propose an efﬁcient deep model, called MotionNet, to jointly perform perception and motion prediction from 3D point clouds. MotionNet takes a sequence of LiDAR sweeps as input and outputs a bird’s eye view (BEV) map, which encodes the object category and motion information in each grid cell. The backbone of MotionNet is a novel spatiotemporal pyramid network, which extracts deep spatial and temporal features in a hierarchical fashion. To enforce the smoothness of predictions over both space and time, the training of MotionNet is further regularized with novel spatial and temporal consistency losses. Extensive experiments show that the proposed method overall outperforms the state-of-the-arts, including the latest scene-ﬂow- and 3D-object-detection-based methods. This indicates the potential value of the proposed method serving as a backup to the bounding-box-based system, and providing complementary information to the motion planner in autonomous driving. Code is available at https://www.merl.com/ research/license\#MotionNet.},
	language = {en},
	urldate = {2025-01-07},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Wu, Pengxiang and Chen, Siheng and Metaxas, Dimitris N.},
	month = jun,
	year = {2020},
	keywords = {notion},
	pages = {11382--11392},
	file = {PDF:E\:\\ZoteroData\\storage\\5GZ7E5H7\\Wu et al. - 2020 - MotionNet Joint Perception and Motion Prediction for Autonomous Driving Based on Bird’s Eye View Ma.pdf:application/pdf},
}

@misc{sadat_perceive_2020,
	title = {Perceive, {Predict}, and {Plan}: {Safe} {Motion} {Planning} {Through} {Interpretable} {Semantic} {Representations}},
	shorttitle = {Perceive, {Predict}, and {Plan}},
	url = {http://arxiv.org/abs/2008.05930},
	doi = {10.48550/arXiv.2008.05930},
	abstract = {In this paper we propose a novel end-to-end learnable network that performs joint perception, prediction and motion planning for self-driving vehicles and produces interpretable intermediate representations. Unlike existing neural motion planners, our motion planning costs are consistent with our perception and prediction estimates. This is achieved by a novel differentiable semantic occupancy representation that is explicitly used as cost by the motion planning process. Our network is learned end-to-end from human demonstrations. The experiments in a large-scale manual-driving dataset and closed-loop simulation show that the proposed model significantly outperforms state-of-the-art planners in imitating the human behaviors while producing much safer trajectories.},
	urldate = {2025-01-10},
	publisher = {arXiv},
	author = {Sadat, Abbas and Casas, Sergio and Ren, Mengye and Wu, Xinyu and Dhawan, Pranaab and Urtasun, Raquel},
	month = aug,
	year = {2020},
	note = {173 citations (Semantic Scholar/arXiv) [2025-01-10]
arXiv:2008.05930 [cs]},
	keywords = {2bread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, notion, Statistics - Machine Learning},
	annote = {Comment: European Conference on Computer Vision (ECCV) 2020},
	file = {Preprint PDF:E\:\\ZoteroData\\storage\\XXGYTKR9\\Sadat et al. - 2020 - Perceive, Predict, and Plan Safe Motion Planning Through Interpretable Semantic Representations.pdf:application/pdf;Snapshot:E\:\\ZoteroData\\storage\\CW9U43QY\\2008.html:text/html},
}

@inproceedings{yang_radarnet_2020,
	address = {Berlin, Heidelberg},
	title = {{RadarNet}: {Exploiting} {Radar} for {Robust} {Perception} of {Dynamic} {Objects}},
	isbn = {978-3-030-58522-8},
	shorttitle = {{RadarNet}},
	url = {https://doi.org/10.1007/978-3-030-58523-5_29},
	doi = {10.1007/978-3-030-58523-5_29},
	abstract = {We tackle the problem of exploiting Radar for perception in the context of self-driving as Radar provides complementary information to other sensors such as LiDAR or cameras in the form of Doppler velocity. The main challenges of using Radar are the noise and measurement ambiguities which have been a struggle for existing simple input or output fusion methods. To better address this, we propose a new solution that exploits both LiDAR and Radar sensors for perception. Our approach, dubbed RadarNet, features a voxel-based early fusion and an attention-based late fusion, which learn from data to exploit both geometric and dynamic information of Radar data. RadarNet achieves state-of-the-art results on two large-scale real-world datasets in the tasks of object detection and velocity estimation. We further show that exploiting Radar improves the perception capabilities of detecting faraway objects and understanding the motion of dynamic objects.},
	urldate = {2025-01-09},
	booktitle = {Computer {Vision} – {ECCV} 2020: 16th {European} {Conference}, {Glasgow}, {UK}, {August} 23–28, 2020, {Proceedings}, {Part} {XVIII}},
	publisher = {Springer-Verlag},
	author = {Yang, Bin and Guo, Runsheng and Liang, Ming and Casas, Sergio and Urtasun, Raquel},
	month = aug,
	year = {2020},
	note = {101 citations (Semantic Scholar/DOI) [2025-01-09]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, notion},
	pages = {496--512},
	annote = {Comment: ECCV 2020},
	file = {Preprint PDF:E\:\\ZoteroData\\storage\\ANABHG6V\\Yang et al. - 2020 - RadarNet Exploiting Radar for Robust Perception of Dynamic Objects.pdf:application/pdf;Snapshot:E\:\\ZoteroData\\storage\\CN26DAIU\\2007.html:text/html},
}

@inproceedings{van_kempen_simulation_based_2021,
	title = {A {Simulation}-based {End}-to-{End} {Learning} {Framework} for {Evidential} {Occupancy} {Grid} {Mapping}},
	url = {https://ieeexplore.ieee.org/document/9575715},
	doi = {10.1109/IV48863.2021.9575715},
	abstract = {Evidential occupancy grid maps (OGMs) are a popular representation of the environment of automated vehicles. Inverse sensor models (ISMs) are used to compute OGMs from sensor data such as lidar point clouds. Geometric ISMs show a limited performance when estimating states in unobserved but inferable areas and have difficulties dealing with ambiguous input. Deep learning-based ISMs face the challenge of limited training data and they often cannot handle uncertainty quantification yet. We propose a deep learning-based framework for learning an OGM algorithm which is both capable of quantifying first- and second-order uncertainty and which does not rely on manually labeled data. Results on synthetic and on real-world data show superiority over other approaches. Source code and datasets are available at https://github.com/ika-rwth-aachen/EviLOG.},
	urldate = {2024-03-15},
	booktitle = {2021 {IEEE} {Intelligent} {Vehicles} {Symposium} ({IV})},
	author = {Van Kempen, Raphael and Lampe, Bastian and Woopen, Timo and Eckstein, Lutz},
	month = jul,
	year = {2021},
	note = {6 citations (Crossref) [2024-08-21]},
	keywords = {/unread, Uncertainty, Inverse problems, Laser radar, Computer Science - Robotics, Neural networks, Training data, Computational modeling, Electrical Engineering and Systems Science - Signal Processing, Intelligent vehicles, notion},
	pages = {934--939},
	annote = {Comment: Accepted to be published as part of the 2021 IEEE Intelligent Vehicles Symposium (IV), Nagoya, Japan, July 11-15, 2021},
	file = {IEEE Xplore Abstract Record:E\:\\ZoteroData\\storage\\Q2U6UQGY\\9575715.html:text/html;IEEE Xplore Full Text PDF:E\:\\ZoteroData\\storage\\B6DUF34A\\Van Kempen et al. - 2021 - A Simulation-based End-to-End Learning Framework f.pdf:application/pdf;Preprint PDF:E\:\\ZoteroData\\storage\\7UYWAFWA\\Kempen et al. - 2021 - A Simulation-based End-to-End Learning Framework for Evidential Occupancy Grid Mapping.pdf:application/pdf;Snapshot:E\:\\ZoteroData\\storage\\HQK28FT4\\2102.html:text/html},
}

@inproceedings{xu_rpfa_net_2021,
	title = {{RPFA}-net: a {4D} {RaDAR} pillar feature attention network for {3D} object detection},
	shorttitle = {{RPFA}-net},
	url = {https://ieeexplore.ieee.org/document/9564754},
	doi = {10.1109/ITSC48978.2021.9564754},
	abstract = {3D object detection is a crucial problem in environmental perception for autonomous driving. Currently, most works focused on LiDAR, camera, or their fusion, while very few algorithms involve a RaDAR sensor, especially 4D RaDAR providing 3D position and velocity information. 4D RaDAR can work well in bad weather and has a higher performance than traditional 3D RaDAR, but it also contains lots of noise information and suffers measurement ambiguities. Existing 3D object detection methods can't judge the heading of objects by focusing on local features in sparse point clouds. To better overcome this problem, we propose a new method named RPFA-Net only using a 4D RaDAR, which utilizes a self-attention mechanism instead of PointNet to extract point clouds' global features. These global features containing long-distance information can effectively improve the network's ability to regress the heading angle of objects and enhance detection accuracy. Our method's performance is enhanced by 8.13\% of 3D mAP and 5.52\% of BEV mAP compared with the baseline. Extensive experiments show that RPFA-Net surpasses state-of-the-art 3D detection methods on Astyx HiRes 2019 dataset. The code and pre-trained models are available at https://github.com/adept-thu/RPFA-Net.git.},
	language = {en},
	urldate = {2025-02-26},
	booktitle = {2021 {IEEE} {International} {Intelligent} {Transportation} {Systems} {Conference} (itsc)},
	author = {Xu, Baowei and Zhang, Xinyu and Wang, Li and Hu, Xiaomei and Li, Zhiwei and Pan, Shuyue and Li, Jun and Deng, Yongqiang},
	month = sep,
	year = {2021},
	keywords = {Meteorological radar, Object detection, Position measurement, Radar, Radar detection, Radar measurements, Three-dimensional displays},
	pages = {3061--3066},
	file = {Full Text PDF:E\:\\ZoteroData\\storage\\UVXCM2Q2\\Xu et al. - 2021 - RPFA-Net a 4D RaDAR Pillar Feature Attention Network for 3D Object Detection.pdf:application/pdf},
}
